# README.md

# Описание проекта

# Этот проект демонстрирует:

1. Создание фиксированного MLP (Multilayer Perceptron) с предопределенными весами с использованием PyTorch.
2. Генерацию датасета путем пропуска случайных входных данных через фиксированный MLP.
3. Обучение нового MLP (MLP2) на сгенерированном датасете для приближения поведения фиксированного MLP.

# Требования

- Python 3.x
- PyTorch
- scikit-learn

# Установка

1. Клонируйте репозиторий или скопируйте файлы проекта в вашу рабочую директорию.

2. Создайте виртуальное окружение (рекомендуется):

   python -m venv venv

3. Активируйте виртуальное окружение:

  - Windows:

      venv\Scripts\activate

    - macOS/Linux:

      source venv/bin/activate

 4. Установите необходимые пакеты:

    pip install -r requirements.txt

# Запуск кода

# Выполните Python-скрипт:

    python ваш_скрипт.py

# Замените `ваш_скрипт.py` на имя вашего файла с кодом.

# Описание проекта

 ### 1. FixedMLP

 - Описание: Нейронная сеть с фиксированными весами, инициализированными в диапазоне [-1, 1].
 - Архитектура:
   - Входной слой: 10 признаков → 20 нейронов.
   - Функция активации: ReLU.
   - Выходной слой: 1 нейрон.

# ### 2. Генерация данных

# - Процесс:
   - Генерируется 1000 образцов случайных входных данных (тензор размером [1000, 10]).
   - Входные данные проходят через FixedMLP для получения выходных значений.
   - Полученный датасет разделяется на обучающую (80%) и тестовую (20%) выборки.

# ### 3. MLP2

 - Описание: Новая нейронная сеть с той же архитектурой, что и у FixedMLP, но с случайной инициализацией весов.
 - Цель: Обучить MLP2 на сгенерированных данных, чтобы она могла приблизить поведение FixedMLP.
 - Обучение:
   - Функция потерь: Mean Squared Error (MSE).
   - Оптимизатор: Adam с коэффициентом обучения lr=0.001.
   - Количество эпох: 100.
 - Оценка:
   - После обучения модель оценивается на тестовом наборе данных.
   - Выводится значение функции потерь на тестовых данных.

# Результаты

# - Обучение: Модель MLP2 успешно обучается приближать поведение фиксированной модели FixedMLP.
# - Функция потерь: Низкое значение MSE на тестовых данных указывает на хорошее качество обучения.

# Пример вывода

 Epoch [10/100], Loss: 0.0523
 Epoch [20/100], Loss: 0.0341
 ...
 Epoch [100/100], Loss: 0.0012
 Test Loss: 0.0015

# Дополнительная информация

 - Расширение:
   - Можно изменять архитектуру сети (количество слоев, нейронов) и наблюдать, как это влияет на обучение.
   - Можно использовать разные функции активации или оптимизаторы.
 - Визуализация:
   - Для более глубокого анализа можно добавить визуализацию функции потерь или распределения предсказаний.

# Лицензия

 Этот проект лицензирован под лицензией MIT License.
